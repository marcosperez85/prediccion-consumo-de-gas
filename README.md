# ğŸ­ Industrial Time Series Forecasting

Pipeline completo y modular para predecir valores futuros en series temporales industriales. EstÃ¡ diseÃ±ado como un template reutilizable para experimentaciÃ³n, benchmarking de modelos y despliegue rÃ¡pido de prototipos.

## ğŸ¯ Objetivo

Desarrollar un sistema de predicciÃ³n genÃ©rico para series temporales industriales que permita:
- ExperimentaciÃ³n rÃ¡pida con diferentes algoritmos
- Feature engineering avanzado
- ComparaciÃ³n objetiva de modelos
- Despliegue a travÃ©s de API REST
- VisualizaciÃ³n y anÃ¡lisis de resultados

## âœ¨ CaracterÃ­sticas

- ğŸ”§ **Feature Engineering Avanzado**: CreaciÃ³n automÃ¡tica de features temporales, lags, rolling statistics e interacciones
- ğŸ¤– **MÃºltiples Modelos**: Linear Regression, Random Forest, Gradient Boosting con comparaciÃ³n automÃ¡tica
- ğŸš€ **API REST**: FastAPI para servir predicciones en producciÃ³n
- ğŸ“Š **MÃ©tricas Completas**: MSE, RMSE, MAE, MAPE, RÂ² para evaluaciÃ³n exhaustiva
- ğŸ“ˆ **VisualizaciÃ³n**: Notebooks con anÃ¡lisis exploratorio y comparaciÃ³n de modelos
- ğŸ”„ **Reproducibilidad**: Scripts automatizados para entrenamiento y evaluaciÃ³n
- ğŸ—ï¸ **OrganizaciÃ³n MLOps**: Estructura limpia para proyectos de ML en producciÃ³n

## ğŸ—ï¸ Arquitectura del Proyecto

```
industrial-time-series-forecasting/
â”‚
â”œâ”€â”€ data/                                    # Datos del proyecto
â”‚   â”œâ”€â”€ raw/                                # Datos sin procesar
â”‚   â”‚   â””â”€â”€ industrial_timeseries.csv      # Dataset sintÃ©tico generado
â”‚   â””â”€â”€ processed/                          # Datos procesados
â”‚       â”œâ”€â”€ industrial_timeseries_featured.csv  # Con feature engineering
â”‚       â””â”€â”€ future_predictions.csv         # Predicciones futuras
â”‚
â”œâ”€â”€ models/                                 # Modelos entrenados
â”‚   â”œâ”€â”€ best_model.pkl                     # Mejor modelo segÃºn mÃ©tricas
â”‚   â”œâ”€â”€ model_info.pkl                     # Metadatos del modelo
â”‚   â”œâ”€â”€ linear_regression_model.pkl        # Modelo de regresiÃ³n lineal
â”‚   â”œâ”€â”€ random_forest_model.pkl           # Modelo Random Forest
â”‚   â””â”€â”€ gradient_boosting_model.pkl       # Modelo Gradient Boosting
â”‚
â”œâ”€â”€ notebooks/                             # AnÃ¡lisis y experimentaciÃ³n
â”‚   â”œâ”€â”€ 01_load.ipynb                     # Carga y validaciÃ³n de datos
â”‚   â”œâ”€â”€ 02_eda.ipynb                      # AnÃ¡lisis exploratorio
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb      # CreaciÃ³n de caracterÃ­sticas
â”‚   â”œâ”€â”€ 04_model.ipynb                    # Entrenamiento y evaluaciÃ³n
â”‚   â””â”€â”€ 05_forecast.ipynb                 # Predicciones futuras
â”‚
â””â”€â”€ src/                                   # CÃ³digo fuente
    â”œâ”€â”€ create_dataset.py                  # GeneraciÃ³n de datos sintÃ©ticos
    â”œâ”€â”€ load.py                           # Funciones de carga de datos
    â”œâ”€â”€ train_model.py                    # Entrenamiento de modelos
    â”œâ”€â”€ model_compare.py                  # ComparaciÃ³n de modelos
    â”œâ”€â”€ predict.py                        # Funciones de predicciÃ³n
    â””â”€â”€ main_api.py                       # API FastAPI
```

## ğŸ“Š Dataset

El dataset sintÃ©tico incluye 6 features principales que representan variables comunes en entornos industriales:

### Features Principales
- **`timestamp`**: Marca temporal horaria
- **`value`**: Variable objetivo (producciÃ³n/consumo industrial)
- **`temperature`**: Temperatura ambiente (Â°C) con estacionalidad
- **`demand_factor`**: Factor de demanda del mercado (0-1)
- **`operational_efficiency`**: Eficiencia operacional (0-1)
- **`energy_price`**: Precio de energÃ­a ($/MWh)

### Features Derivadas (Feature Engineering)
- Variables temporales: `hour`, `day_of_week`, `month`, `is_weekend`
- Lags: `lag_1h`, `lag_24h`, `lag_168h`
- Rolling statistics: `rolling_mean_24h`, `rolling_std_24h`
- Interacciones: `demand_efficiency_interaction`, `temp_demand_interaction`
- Transformaciones: `temp_squared`

## ğŸš€ Inicio RÃ¡pido

### 1. Generar Dataset
```bash
cd src
python create_dataset.py
```

### 2. Ejecutar Notebooks (Orden recomendado)
1. `01_load.ipynb` - Cargar y explorar datos iniciales
2. `02_eda.ipynb` - AnÃ¡lisis exploratorio detallado
3. `03_feature_engineering.ipynb` - Crear features avanzadas
4. `04_model.ipynb` - Entrenar y comparar modelos
5. `05_forecast.ipynb` - Generar predicciones futuras

### 3. Entrenar Modelos por CLI

#### Entrenar modelo individual:
```bash
cd src

# Random Forest (recomendado)
python train_model.py --model random_forest

# Linear Regression
python train_model.py --model linear_regression

# Gradient Boosting
python train_model.py --model gradient_boosting
```

#### Comparar mÃºltiples modelos:
```bash
# Solo comparar (no guardar)
python model_compare.py

# Comparar y guardar mejor modelo
python model_compare.py --save
```

### 4. Usar API REST

#### Iniciar servidor:
```bash
cd src
python main_api.py
```

#### Realizar predicciÃ³n:
```bash
curl -X POST "http://localhost:8000/predict" \
-H "Content-Type: application/json" \
-d '{
  "temperature": 22.5,
  "demand_factor": 0.75,
  "operational_efficiency": 0.85,
  "energy_price": 85.0,
  "hour": 14,
  "day_of_week": 2,
  "month": 6,
  "is_weekend": 0,
  "lag_1h": 1150.0,
  "lag_24h": 1180.0,
  "rolling_mean_24h": 1165.0,
  "rolling_std_24h": 25.0
}'
```

#### Endpoints disponibles:
- `GET /` - InformaciÃ³n general
- `POST /predict` - Realizar predicciÃ³n
- `GET /health` - Estado del servicio
- `GET /model-info` - InformaciÃ³n del modelo cargado

## ğŸ”§ Uso del CÃ³digo

### PredicciÃ³n Individual
```python
from src.predict import predict_single

result = predict_single(
    temperature=22.5,
    demand_factor=0.75,
    operational_efficiency=0.85,
    energy_price=85.0,
    hour=14
)
print(f"PredicciÃ³n: {result['prediction']}")
```

### PredicciÃ³n en Lote
```python
from src.predict import predict_batch
import pandas as pd

df = pd.read_csv("your_data.csv")
predictions = predict_batch(df)
```

### Cargar Datos
```python
from src.load import load_data, load_processed_data

# Datos originales
df_raw = load_data()

# Datos con feature engineering
df_processed = load_processed_data()
```

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

El sistema evalÃºa modelos usando mÃºltiples mÃ©tricas:

- **MAE** (Mean Absolute Error): Error promedio absoluto
- **RMSE** (Root Mean Square Error): Penaliza errores grandes
- **RÂ²** (R-squared): ProporciÃ³n de varianza explicada
- **MAPE** (Mean Absolute Percentage Error): Error porcentual promedio

## ğŸ› ï¸ PersonalizaciÃ³n

### Agregar Nuevos Modelos
Edita `model_compare.py` o `train_model.py`:

```python
from sklearn.svm import SVR

models = {
    "LinearRegression": LinearRegression(),
    "RandomForest": RandomForestRegressor(n_estimators=100),
    "SVM": SVR(kernel='rbf')  # Nuevo modelo
}
```

### Modificar Features
Edita la lista `feature_columns` en los scripts:

```python
feature_columns = [
    'temperature', 'demand_factor', 'operational_efficiency',
    'your_new_feature'  # Nueva feature
]
```

### Personalizar Dataset
Modifica `create_dataset.py` para generar datos especÃ­ficos para tu dominio.

## ğŸ“‹ Dependencias

```txt
pandas>=1.5.0
numpy>=1.21.0
scikit-learn>=1.1.0
matplotlib>=3.5.0
seaborn>=0.11.0
fastapi>=0.85.0
uvicorn>=0.18.0
joblib>=1.1.0
pydantic>=1.10.0
```

## ğŸ¤ Contribuciones

1. Fork el proyecto
2. Crea una branch para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Notas de Desarrollo

- Los modelos se guardan automÃ¡ticamente en `models/`
- El split de datos es temporal (80% entrenamiento, 20% prueba)
- Las features de lag requieren datos histÃ³ricos
- La API incluye validaciÃ³n automÃ¡tica de inputs
- Todos los scripts incluyen manejo de errores

## ğŸ”® PrÃ³ximas CaracterÃ­sticas

- [ ] Modelos de deep learning (LSTM, GRU)
- [ ] DetecciÃ³n automÃ¡tica de anomalÃ­as
- [ ] Dashboard interactivo con Streamlit
- [ ] ContainerizaciÃ³n con Docker
- [ ] Pipeline de CI/CD
- [ ] Monitoreo de deriva de datos
- [ ] Explicabilidad de modelos (SHAP)

---